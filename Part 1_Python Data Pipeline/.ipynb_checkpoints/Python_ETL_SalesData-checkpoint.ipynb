{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2707d57c",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc2f400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import warnings\n",
    "\n",
    "# Database connection\n",
    "import pyodbc  # SQL Server\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911571f1",
   "metadata": {},
   "source": [
    "## Logging to file configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "896cee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for logs if it doesn't exist\n",
    "log_dir = \"code_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Define timestamp for this ETL run\n",
    "etl_run_id = datetime.now()\n",
    "#print(etl_run_id)\n",
    "\n",
    "# Log file name\n",
    "log_file = os.path.join(log_dir, f\"sales_etl_code_execution_logs.log\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Helper function to log and print together\n",
    "def log_print(message, level=\"info\"):\n",
    "    if level == \"info\":\n",
    "        logging.info(message)\n",
    "        print(message)\n",
    "    elif level == \"error\":\n",
    "        logging.error(message)\n",
    "        print(\"ERROR:\", message)\n",
    "        \n",
    "logging.info(\"ETL job started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83689966",
   "metadata": {},
   "source": [
    "## SQL Server Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c783e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# SQL Server Connection\n",
    "# -----------------------------\n",
    "sql_conn_str = (\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=localhost;\"\n",
    "    \"DATABASE=SalesDB;\"\n",
    "    \"Trusted_Connection=yes;\"\n",
    ")\n",
    "\n",
    "# Create a connection object\n",
    "try:\n",
    "    conn = pyodbc.connect(sql_conn_str)\n",
    "    cursor = conn.cursor()\n",
    "    logging.info(\"Successfully connected to SalesDB.\")\n",
    "    #print(\"Successfully connected to SalesDB.\")\n",
    "except Exception as e:\n",
    "    logging.info(f\"Error connecting to SQL Server: {e}\")\n",
    "    #print(f\"Error connecting to SQL Server: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2aed60",
   "metadata": {},
   "source": [
    "## Logging bad data to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4095f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_bad_data(df_rows, etl_run_id, source_system, bad_column, issue_type, notes_prefix, conn_str):\n",
    "    \"\"\"\n",
    "    Logs bad data rows into the BadDataLog table.\n",
    "\n",
    "    Parameters:\n",
    "    - df_rows: DataFrame containing rows to log\n",
    "    - etl_run_id: timestamp of ETL run\n",
    "    - source_system: e.g., 'python_etl', 'ssis_pkg'\n",
    "    - bad_column: name of the column with issue, or 'ALL_COLUMNS' for full duplicates\n",
    "    - issue_type: type of issue (missing, duplicate, invalid_format, etc.)\n",
    "    - notes_prefix: optional prefix to notes\n",
    "    - conn: existing pyodbc connection object\n",
    "    \"\"\"\n",
    "    bad_data_list = []\n",
    "    \n",
    "    for idx, row in df_rows.iterrows():\n",
    "        record_id = row['transaction_id'] if 'transaction_id' in row else None\n",
    "        bad_data_list.append({\n",
    "            'etl_run_id': etl_run_id,\n",
    "            'source_system': source_system,\n",
    "            'record_id': record_id,\n",
    "            'bad_column': bad_column,\n",
    "            'issue_type': issue_type,\n",
    "            'original_record': row.to_json(),\n",
    "            'notes': notes_prefix\n",
    "        })\n",
    "    \n",
    "    if bad_data_list:\n",
    "        bad_data_df = pd.DataFrame(bad_data_list)\n",
    "        cursor = conn.cursor()\n",
    "        for _, log_row in bad_data_df.iterrows():\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO dbo.BadDataLog\n",
    "                (etl_run_id, source_system, record_id, bad_column, issue_type, original_record, notes)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            log_row['etl_run_id'],\n",
    "            log_row['source_system'],\n",
    "            log_row['record_id'],\n",
    "            log_row['bad_column'],\n",
    "            log_row['issue_type'],\n",
    "            log_row['original_record'],\n",
    "            log_row['notes'])\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de497e0",
   "metadata": {},
   "source": [
    "## Load JSON file data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1c22013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>discount</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>product.id</th>\n",
       "      <th>product.name</th>\n",
       "      <th>product.category</th>\n",
       "      <th>product.price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T001</td>\n",
       "      <td>C001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>North</td>\n",
       "      <td>P01</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>999.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T002</td>\n",
       "      <td>C002</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-10T14:15:00Z</td>\n",
       "      <td>South</td>\n",
       "      <td>P02</td>\n",
       "      <td>Mouse</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T003</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>03-05-2023</td>\n",
       "      <td>East</td>\n",
       "      <td>P03</td>\n",
       "      <td>Monitor</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>299.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T004</td>\n",
       "      <td>C004</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2023-04-20</td>\n",
       "      <td>West</td>\n",
       "      <td>P04</td>\n",
       "      <td>Keyboard</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>49.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T005</td>\n",
       "      <td>C001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2023/05/12 12:30:00</td>\n",
       "      <td>North</td>\n",
       "      <td>P05</td>\n",
       "      <td>Desk</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>189.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T00006</td>\n",
       "      <td>C064</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>2023-02-12</td>\n",
       "      <td>West</td>\n",
       "      <td>P03</td>\n",
       "      <td>Monitor 4K</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>299.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T00007</td>\n",
       "      <td>C054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2023-03-20T02:12:32Z</td>\n",
       "      <td>West</td>\n",
       "      <td>P03</td>\n",
       "      <td>Monitor 4K</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>299.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T00008</td>\n",
       "      <td>C034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023/09/08 19:26:58</td>\n",
       "      <td>East</td>\n",
       "      <td>P02</td>\n",
       "      <td>Mouse</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T00009</td>\n",
       "      <td>C065</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>26-03-2023</td>\n",
       "      <td>South</td>\n",
       "      <td>P01</td>\n",
       "      <td>Laptop Pro+</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>999.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T00010</td>\n",
       "      <td>C051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2023-05-26T00:50:45Z</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>P02</td>\n",
       "      <td>Mouse</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>19.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id customer_id  quantity  discount                  date  \\\n",
       "0           T001        C001       2.0      0.05                  None   \n",
       "1           T002        C002       5.0       NaN  2023-02-10T14:15:00Z   \n",
       "2           T003        None      -1.0      0.10            03-05-2023   \n",
       "3           T004        C004       4.0      0.15            2023-04-20   \n",
       "4           T005        C001       3.0      0.00   2023/05/12 12:30:00   \n",
       "5         T00006        C064       5.0     -0.05            2023-02-12   \n",
       "6         T00007        C054       0.0      0.00  2023-03-20T02:12:32Z   \n",
       "7         T00008        C034       NaN       NaN   2023/09/08 19:26:58   \n",
       "8         T00009        C065      -1.0      0.10            26-03-2023   \n",
       "9         T00010        C051       0.0      0.15  2023-05-26T00:50:45Z   \n",
       "\n",
       "    region product.id product.name product.category  product.price  \n",
       "0    North        P01       Laptop      Electronics         999.99  \n",
       "1    South        P02        Mouse      Accessories          19.99  \n",
       "2     East        P03      Monitor      Electronics         299.50  \n",
       "3     West        P04     Keyboard      Accessories          49.90  \n",
       "4    North        P05         Desk        Furniture         189.00  \n",
       "5     West        P03   Monitor 4K      Electronics         299.50  \n",
       "6     West        P03   Monitor 4K      Electronics         299.50  \n",
       "7     East        P02        Mouse      Accessories          19.99  \n",
       "8    South        P01  Laptop Pro+      Electronics         999.99  \n",
       "9  Unknown        P02        Mouse      Accessories          19.99  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    input_file = 'sales_data.json'\n",
    "    logging.info(f\"Loading input JSON file: {input_file}\")\n",
    "    \n",
    "    # Read JSON data\n",
    "    with open(input_file, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    sales_df = pd.json_normalize(raw_data)\n",
    "    logging.info(f\"Total records loaded: {len(sales_df)}\")\n",
    "    \n",
    "    # Optional: preview first 5 rows\n",
    "    #print(\"Total records loaded:\", len(sales_df))\n",
    "    display(sales_df.head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading JSON data: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732f253",
   "metadata": {},
   "source": [
    "## Data validation, cleansing, and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8619de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove exact duplicate rows\n",
    "try:\n",
    "    logging.info(\"Starting duplicate rows removal step.\")\n",
    "    \n",
    "    duplicate_rows = sales_df[sales_df.duplicated(keep='first')]\n",
    "    log_bad_data(\n",
    "        df_rows=duplicate_rows,\n",
    "        etl_run_id=etl_run_id,\n",
    "        source_system='python_etl',\n",
    "        bad_column='ALL_COLUMNS',\n",
    "        issue_type='duplicate_full_row',\n",
    "        notes_prefix='Dropped this full duplicate row',\n",
    "        conn_str=sql_conn_str\n",
    "    ) \n",
    "    \n",
    "    initial_count = len(sales_df)\n",
    "    sales_df = sales_df.drop_duplicates()\n",
    "    removed_count = initial_count - len(sales_df)\n",
    "    \n",
    "    logging.info(f\"Dropped {removed_count} exact duplicate rows.\")\n",
    "    logging.info(f\"Remaining records after duplicate removal: {len(sales_df)}\")\n",
    "    \n",
    "    #print(f\"Dropped {removed_count} duplicate rows. Remaining records: {len(sales_df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during duplicate removal: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a63ac394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Clean 'transaction_id' column ----\n",
    "try:\n",
    "    logging.info(\"Starting 'transaction_id' cleaning step.\")\n",
    "\n",
    "    # Strip whitespace and ensure string type\n",
    "    sales_df['transaction_id'] = sales_df['transaction_id'].astype(str).str.strip()\n",
    "    logging.info(\"Transaction IDs converted to string and stripped.\")\n",
    "\n",
    "    # Identify missing or blank transaction Ids\n",
    "    missing_or_blank_mask = (\n",
    "    sales_df['transaction_id'].isna() |\n",
    "    (sales_df['transaction_id'].str.strip() == '') |\n",
    "    (sales_df['transaction_id'].str.lower().isin(['nan', 'none']))\n",
    "    )\n",
    "    #missing_or_blank_mask = (sales_df['transaction_id'].isna()) | (sales_df['transaction_id'] == '') | (sales_df['transaction_id'].str.lower() == 'nan')\n",
    "    missing_or_blank_rows = sales_df[missing_or_blank_mask]\n",
    "\n",
    "    if not missing_or_blank_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=missing_or_blank_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='transaction_id',\n",
    "            issue_type='missing_or_blank',\n",
    "            notes_prefix='Dropped row due to missing or blank transaction_id',\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged and dropped {len(missing_or_blank_rows)} rows with missing/blank transaction_id.\")\n",
    "    \n",
    "    # Drop missing/blank transaction Id\n",
    "    sales_df = sales_df[~missing_or_blank_mask]\n",
    "\n",
    "    # Identify duplicate transaction Id + product Id\n",
    "    duplicate_mask = sales_df.duplicated(subset=['transaction_id', 'product.id'], keep='first')\n",
    "    duplicate_rows = sales_df[duplicate_mask]\n",
    "\n",
    "    if not duplicate_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=duplicate_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='transaction_id',\n",
    "            issue_type='duplicate_transactionid+productid',\n",
    "            notes_prefix='Dropped duplicate transactionid+productid pairs',\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged and dropped {len(duplicate_rows)} duplicate transactionid + productid pairs rows.\")\n",
    "    \n",
    "    # Drop duplicate transaction Id + product Id\n",
    "    sales_df = sales_df.drop_duplicates(subset=['transaction_id', 'product.id'], keep='first')\n",
    "\n",
    "    # Log completion\n",
    "    logging.info(f\"Remaining records after transaction_id cleaning: {len(sales_df)}\")\n",
    "    #print(f\"transaction_id cleaning complete. Remaining records: {len(sales_df)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during 'transaction_id' cleaning: {e}\", exc_info=True)\n",
    "    print(f\"❌ Error during 'transaction_id' cleaning: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41f67bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Clean 'customer_id' column ----\n",
    "try:\n",
    "    logging.info(\"Starting 'customer_id' cleaning step.\")\n",
    "\n",
    "    # Convert to string and strip whitespace\n",
    "    sales_df['customer_id'] = sales_df['customer_id'].astype(str).str.strip()\n",
    "    logging.info(f\"Total records after stripping whitespace: {len(sales_df)}\")\n",
    "\n",
    "    # Treat empty strings or placeholders as missing\n",
    "    sales_df['customer_id'] = sales_df['customer_id'].replace(['', 'None', 'nan'], None)\n",
    "    logging.info(\"Replaced empty/placeholder values in 'customer_id' with None.\")\n",
    "\n",
    "    # Identify missing customer_id rows before filling\n",
    "    missing_customer_rows = sales_df[sales_df['customer_id'].isna()]\n",
    "\n",
    "    if not missing_customer_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=missing_customer_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='customer_id',\n",
    "            issue_type='missing',\n",
    "            notes_prefix='customer_id missing or blank; replaced with \"Unknown\"',\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged {len(missing_customer_rows)} records with missing customer_id before replacement.\")\n",
    "\n",
    "    # Fill missing customer_id with placeholder 'Unknown'\n",
    "    sales_df['customer_id'] = sales_df['customer_id'].fillna('Unknown')\n",
    "    num_unknown = (sales_df['customer_id'] == 'Unknown').sum()\n",
    "    logging.info(f\"Number of records with missing customer_id replaced by Unknown: {num_unknown}\")\n",
    "\n",
    "    # Confirm total rows remain unchanged\n",
    "    logging.info(f\"Total records after 'customer_id' cleaning: {len(sales_df)}\")\n",
    "    #print(f\"Number of records with missing customer_id replaced: {num_unknown}\")\n",
    "    #print(f\"Total records after 'customer_id' cleaning: {len(sales_df)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during 'customer_id' cleaning: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9e1e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Clean 'product' columns ----\n",
    "try:\n",
    "    logging.info(\"Starting 'product' columns cleaning steps.\")\n",
    "\n",
    "    # --- Product ID ---\n",
    "    sales_df['product_id'] = sales_df['product.id'].astype(str).str.strip()\n",
    "\n",
    "    # Identify missing/placeholder product_id rows before replacement\n",
    "    missing_product_id_rows = sales_df[sales_df['product_id'].isin(['', 'None', 'nan'])]\n",
    "    if not missing_product_id_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=missing_product_id_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='product_id',\n",
    "            issue_type='missing',\n",
    "            notes_prefix=\"product_id missing or blank; replaced with 'Unknown'\",\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged {len(missing_product_id_rows)} missing product_id rows.\")\n",
    "\n",
    "    sales_df['product_id'] = sales_df['product_id'].replace(['', 'None', 'nan'], 'Unknown')\n",
    "    num_unknown_product = (sales_df['product_id'] == 'Unknown').sum()\n",
    "    logging.info(f\"Number of records with missing product_id replaced with 'Unknown': {num_unknown_product}\")\n",
    "    #print(f\"Number of records with missing product_id replaced with 'Unknown': {num_unknown_product}\")\n",
    "\n",
    "    # --- Product Name ---\n",
    "    sales_df['product_name'] = sales_df['product.name'].astype(str).str.strip()\n",
    "\n",
    "    missing_pname_rows = sales_df[sales_df['product_name'].isin(['', 'None', 'nan'])]\n",
    "    if not missing_pname_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=missing_pname_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='product_name',\n",
    "            issue_type='missing',\n",
    "            notes_prefix=\"product_name missing or blank; replaced with 'Unknown'\",\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged {len(missing_pname_rows)} missing product_name rows.\")\n",
    "\n",
    "    sales_df['product_name'] = sales_df['product_name'].replace(['', 'None', 'nan'], 'Unknown')\n",
    "    num_unknown_pname = (sales_df['product_name'] == 'Unknown').sum()\n",
    "    logging.info(f\"Number of records with missing/invalid product name replaced with 'Unknown': {num_unknown_pname}\")\n",
    "    #print(f\"Number of records with missing/invalid product name replaced with 'Unknown': {num_unknown_pname}\")\n",
    "\n",
    "    # --- Category ---\n",
    "    sales_df['category'] = sales_df['product.category'].astype(str).str.strip()\n",
    "\n",
    "    missing_category_rows = sales_df[sales_df['category'].isin(['', 'None', 'nan', 'NaN'])]\n",
    "    if not missing_category_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=missing_category_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='category',\n",
    "            issue_type='missing',\n",
    "            notes_prefix=\"category missing or blank; replaced with 'Unknown'\",\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged {len(missing_category_rows)} missing category rows.\")\n",
    "\n",
    "    sales_df['category'] = sales_df['category'].replace(['', 'None', 'nan', 'NaN'], 'Unknown')\n",
    "    num_unknown_category = (sales_df['category'] == 'Unknown').sum()\n",
    "    logging.info(f\"Number of records with missing/invalid category replaced with 'Unknown': {num_unknown_category}\")\n",
    "    #print(f\"Number of records with missing/invalid category replaced with 'Unknown': {num_unknown_category}\")\n",
    "\n",
    "    # --- Price ---\n",
    "    sales_df['price'] = pd.to_numeric(sales_df['product.price'], errors='coerce')\n",
    "\n",
    "    # Identify missing/NaN or negative price rows\n",
    "    bad_price_rows = sales_df[sales_df['price'].isna() | (sales_df['price'] < 0)]\n",
    "    if not bad_price_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=bad_price_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='price',\n",
    "            issue_type='invalid_value',\n",
    "            notes_prefix=\"price missing, non-numeric, or negative; replaced with 0\",\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged {len(bad_price_rows)} invalid/missing price rows.\")\n",
    "\n",
    "    sales_df['price'] = sales_df['price'].fillna(0)\n",
    "    neg_price_count = (sales_df['price'] < 0).sum()\n",
    "    sales_df.loc[sales_df['price'] < 0, 'price'] = 0\n",
    "    logging.info(f\"Replaced {neg_price_count} negative price values with 0\")\n",
    "    #print(f\"Replaced {neg_price_count} negative price values with 0\")\n",
    "    \n",
    "    logging.info(f\"Total records after 'product' columns cleaning: {len(sales_df)}\")\n",
    "    #print(f\"Total records after 'product' columns cleaning: {len(sales_df)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during product data cleaning: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6a32a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Clean 'quantity' and 'discount' columns ----\n",
    "try:\n",
    "    logging.info(\"Starting 'quantity' and 'discount' columns cleaning steps.\")\n",
    "\n",
    "    # --- Quantity ---\n",
    "    sales_df['quantity'] = pd.to_numeric(sales_df['quantity'], errors='coerce')\n",
    "\n",
    "    # Identify missing or negative quantity rows before fixing\n",
    "    bad_quantity_rows = sales_df[sales_df['quantity'].isna() | (sales_df['quantity'] < 0)]\n",
    "    if not bad_quantity_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=bad_quantity_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='quantity',\n",
    "            issue_type='invalid_value',\n",
    "            notes_prefix=\"quantity missing, non-numeric, or negative; replaced with 0\",\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged {len(bad_quantity_rows)} bad quantity rows.\")\n",
    "\n",
    "    # Replace missing and negative values\n",
    "    missing_quantity_count = sales_df['quantity'].isna().sum()\n",
    "    negative_quantity_count = (sales_df['quantity'] < 0).sum()\n",
    "    sales_df['quantity'] = sales_df['quantity'].fillna(0)\n",
    "    sales_df.loc[sales_df['quantity'] < 0, 'quantity'] = 0\n",
    "\n",
    "    # Log counts\n",
    "    logging.info(f\"Replaced {missing_quantity_count} missing quantity values with 0\")\n",
    "    logging.info(f\"Replaced {negative_quantity_count} negative quantity values with 0\")\n",
    "    #print(f\"Replaced {missing_quantity_count} missing quantity values with 0\")\n",
    "    #print(f\"Replaced {negative_quantity_count} negative quantity values with 0\")\n",
    "\n",
    "    # --- Discount ---\n",
    "    sales_df['discount'] = pd.to_numeric(sales_df['discount'], errors='coerce')\n",
    "\n",
    "    # Identify missing or negative discount rows before fixing\n",
    "    bad_discount_rows = sales_df[sales_df['discount'].isna() | (sales_df['discount'] < 0)]\n",
    "    if not bad_discount_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=bad_discount_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='discount',\n",
    "            issue_type='invalid_value',\n",
    "            notes_prefix=\"discount missing, non-numeric, or negative; replaced with 0\",\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged {len(bad_discount_rows)} bad discount rows.\")\n",
    "\n",
    "    # Replace missing and negative values\n",
    "    missing_discount_count = sales_df['discount'].isna().sum()\n",
    "    negative_discount_count = (sales_df['discount'] < 0).sum()\n",
    "    sales_df['discount'] = sales_df['discount'].fillna(0)\n",
    "    sales_df.loc[sales_df['discount'] < 0, 'discount'] = 0\n",
    "\n",
    "    # Log counts\n",
    "    logging.info(f\"Replaced {missing_discount_count} missing discount values with 0\")\n",
    "    logging.info(f\"Replaced {negative_discount_count} negative discount values with 0\")\n",
    "    #print(f\"Replaced {missing_discount_count} missing discount values with 0\")\n",
    "    #print(f\"Replaced {negative_discount_count} negative discount values with 0\")\n",
    "\n",
    "    logging.info(f\"Total records after 'quantity & discounts' columns cleaning: {len(sales_df)}\")\n",
    "    #print(f\"Total records after 'quantity & discounts' columns cleaning: {len(sales_df)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during quantity/discount cleaning: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79832364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'date' column\n",
    "try:\n",
    "    logging.info(\"Starting date column cleaning and parsing.\")\n",
    "\n",
    "    # Initial info\n",
    "    null_count_before = sales_df['date'].isna().sum()\n",
    "    logging.info(f\"Number of null/missing values in original 'date' column: {null_count_before}\")\n",
    "    #print(f\"Number of null/missing values in original 'date' column: {null_count_before}\")\n",
    "\n",
    "    # Identify bad date rows before parsing\n",
    "    bad_date_rows = sales_df[\n",
    "        sales_df['date'].isna() |\n",
    "        (sales_df['date'].astype(str).str.strip() == '')\n",
    "    ].copy()\n",
    "\n",
    "    # Log bad date rows (missing/blank)\n",
    "    if not bad_date_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=bad_date_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='date',\n",
    "            issue_type='missing_value',\n",
    "            notes_prefix=\"date missing or blank; replaced with 1990-01-01\",\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged {len(bad_date_rows)} missing/blank date rows to BadDataLog.\")\n",
    "\n",
    "    # Define parsing function\n",
    "    def parse_date(val):\n",
    "        if pd.isna(val) or str(val).strip() == '':\n",
    "            return pd.Timestamp('19900101')  # missing -> 1990\n",
    "        for fmt in ('%d-%m-%Y', '%Y/%m/%d', '%d %b %Y', '%Y-%m-%d'):\n",
    "            try:\n",
    "                return pd.to_datetime(val, format=fmt)\n",
    "            except:\n",
    "                continue\n",
    "        # fallback to automatic parsing\n",
    "        try:\n",
    "            return pd.to_datetime(val)\n",
    "        except:\n",
    "            return pd.Timestamp('19900101')\n",
    "\n",
    "    # Count missing/blank before parsing\n",
    "    missing_count = null_count_before + (sales_df['date'].astype(str).str.strip() == '').sum()\n",
    "    logging.info(f\"Number of dates to replace with 19900101 due to missing/blank: {missing_count}\")\n",
    "    #print(f\"Number of dates to replace with 19900101 due to missing/blank: {missing_count}\")\n",
    "\n",
    "    # Parse all dates\n",
    "    parsed_dates = sales_df['date'].apply(parse_date)\n",
    "\n",
    "    # Identify rows that were replaced due to invalid parsing\n",
    "    invalid_date_rows = sales_df[\n",
    "        ~sales_df['date'].isna() &\n",
    "        ~(sales_df['date'].astype(str).str.strip() == '') &\n",
    "        (parsed_dates == pd.Timestamp('19900101'))\n",
    "    ].copy()\n",
    "\n",
    "    # Log invalid date format rows\n",
    "    if not invalid_date_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=invalid_date_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='date',\n",
    "            issue_type='invalid_format',\n",
    "            notes_prefix=\"invalid date format; replaced with 19900101\",\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged {len(invalid_date_rows)} invalid-format date rows to BadDataLog.\")\n",
    "\n",
    "    # Check if parsing introduced any nulls\n",
    "    null_after_parsing = parsed_dates.isna().sum()\n",
    "    logging.info(f\"Number of null values after parsing: {null_after_parsing}\")\n",
    "    #print(f\"Number of null values after parsing: {null_after_parsing}\")\n",
    "\n",
    "    # Update 'date' column with cleaned YYYY-MM-DD strings\n",
    "    sales_df['date'] = parsed_dates.apply(lambda x: x.strftime('%Y%m%d') if pd.notnull(x) else '19900101')\n",
    "\n",
    "    # sample\n",
    "    #print(sales_df[['date']].head(30))\n",
    "\n",
    "    logging.info(\"Date column cleaning and parsing completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during date column cleaning/parsing: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d297d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean 'region' column ----\n",
    "try:\n",
    "    logging.info(\"Starting 'region' data cleaning steps.\")\n",
    "\n",
    "    sales_df['region'] = sales_df['region'].astype(str).str.strip()\n",
    "\n",
    "    # Identify missing or invalid region rows\n",
    "    missing_region_rows = sales_df[sales_df['region'].isin(['', 'None', 'nan', 'NaN'])]\n",
    "    if not missing_region_rows.empty:\n",
    "        log_bad_data(\n",
    "            df_rows=missing_region_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='region',\n",
    "            issue_type='missing',\n",
    "            notes_prefix=\"region missing or blank; replaced with 'Unknown'\",\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "        logging.info(f\"Logged {len(missing_region_rows)} missing region rows.\")\n",
    "\n",
    "    # Replace missing or invalid regions with 'Unknown'\n",
    "    sales_df['region'] = sales_df['region'].replace(['', 'None', 'nan', 'NaN'], 'Unknown')\n",
    "    num_unknown_region = (sales_df['region'] == 'Unknown').sum()\n",
    "    logging.info(f\"Number of records with missing/invalid region replaced with 'Unknown': {num_unknown_region}\")\n",
    "    #print(f\"Number of records with missing/invalid region replaced with 'Unknown': {num_unknown_region}\")\n",
    "\n",
    "    logging.info(f\"Total records after 'region' column cleaning: {len(sales_df)}\")\n",
    "    #print(f\"Total records after 'region' column cleaning: {len(sales_df)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during region data cleaning: {e}\", exc_info=True)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1435d46b",
   "metadata": {},
   "source": [
    "## Dim Product incremental data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63af7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(\"Starting incremental load for 'DimProduct'.\")\n",
    "\n",
    "    \n",
    "    # Prepare product data from cleaned sales_df\n",
    "    # ---------------------------------------\n",
    "    product_df = (\n",
    "        sales_df[['product_id', 'product_name', 'category', 'price']]\n",
    "        .query(\"product_id != 'Unknown'\")                 # remove Unknown records\n",
    "        .drop_duplicates()                                # remove full duplicate rows\n",
    "        .drop_duplicates(subset=['product_id'])           # keep one record per product_id\n",
    "        .copy()\n",
    "    )\n",
    "    logging.info(f\"Extracted {len(product_df)} unique products from source dataframe.\")\n",
    "\n",
    "    # Apply type casting to match DimProduct schema\n",
    "    # ---------------------------------------\n",
    "    product_df['product_id'] = product_df['product_id'].astype(str)\n",
    "    product_df['product_name'] = product_df['product_name'].astype(str)\n",
    "    product_df['category'] = product_df['category'].astype(str)\n",
    "    product_df['price'] = product_df['price'].astype(float)\n",
    "\n",
    "    logging.info(\"Applied type casting on product_df columns.\")\n",
    "\n",
    "    # Fetch existing DimProduct data\n",
    "    # ---------------------------------------\n",
    "    warnings.filterwarnings(\"ignore\", message=\"pandas only support SQLAlchemy\")\n",
    "    existing_products = pd.read_sql(\"SELECT product_id, product_name, category, price FROM dbo.DimProduct\", conn)\n",
    "    logging.info(f\"Fetched {len(existing_products)} existing products from DimProduct.\")\n",
    "\n",
    "    # Identify new and updated records\n",
    "    # ---------------------------------------\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    new_products = product_df[~product_df['product_id'].isin(existing_products['product_id'])]\n",
    "    merged = product_df.merge(existing_products, on='product_id', suffixes=('', '_old'))\n",
    "\n",
    "    changed_products = merged[\n",
    "        (merged['product_name'] != merged['product_name_old']) |\n",
    "        (merged['category'] != merged['category_old']) |\n",
    "        (merged['price'] != merged['price_old'])\n",
    "    ][['product_id', 'product_name', 'category', 'price']]\n",
    "\n",
    "    logging.info(f\"New: {len(new_products)} | Updated: {len(changed_products)}\")\n",
    "\n",
    "    # Insert new products\n",
    "    # ---------------------------------------\n",
    "    if not new_products.empty:\n",
    "        insert_sql = \"\"\"\n",
    "            INSERT INTO dbo.DimProduct (product_id, product_name, category, price, created_at, updated_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        for _, row in new_products.iterrows():\n",
    "            cursor.execute(insert_sql, row['product_id'], row['product_name'],\n",
    "                           row['category'], row['price'], current_time, current_time)\n",
    "        logging.info(f\"Inserted {len(new_products)} new products.\")\n",
    "\n",
    "    # Update existing products (if changed)\n",
    "    # ---------------------------------------\n",
    "    if not changed_products.empty:\n",
    "        update_sql = \"\"\"\n",
    "            UPDATE dbo.DimProduct\n",
    "            SET product_name = ?, category = ?, price = ?, updated_at = ?\n",
    "            WHERE product_id = ?\n",
    "        \"\"\"\n",
    "        for _, row in changed_products.iterrows():\n",
    "            cursor.execute(update_sql, row['product_name'], row['category'],\n",
    "                           row['price'], current_time, row['product_id'])\n",
    "        logging.info(f\"Updated {len(changed_products)} existing products.\")\n",
    "\n",
    "    # Commit\n",
    "    # ---------------------------------------\n",
    "    conn.commit()\n",
    "    logging.info(\"DimProduct incremental load committed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during DimProduct incremental load: {e}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e1a64",
   "metadata": {},
   "source": [
    "## Dim Date incremental data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fa93600",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(\"Starting 'DimDate' population process...\")\n",
    "\n",
    "    # Define date range\n",
    "    # -----------------------------\n",
    "    start_date = \"2010-01-01\"\n",
    "    end_date = \"2030-12-31\"\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    # Build DimDate DataFrame\n",
    "    # -----------------------------\n",
    "    dim_date_df = pd.DataFrame({\n",
    "        \"full_date\": date_range\n",
    "    })\n",
    "\n",
    "    dim_date_df[\"date_key\"] = dim_date_df[\"full_date\"].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    dim_date_df[\"year\"] = dim_date_df[\"full_date\"].dt.year\n",
    "    dim_date_df[\"month\"] = dim_date_df[\"full_date\"].dt.month\n",
    "    dim_date_df[\"month_name\"] = dim_date_df[\"full_date\"].dt.strftime(\"%B\")\n",
    "    dim_date_df[\"day\"] = dim_date_df[\"full_date\"].dt.day\n",
    "    dim_date_df[\"quarter\"] = dim_date_df[\"full_date\"].dt.quarter\n",
    "\n",
    "    logging.info(f\"Generated DimDate DataFrame with {len(dim_date_df)} from {start_date} to {end_date}\")\n",
    "\n",
    "    # Check if DimDate already populated\n",
    "    # -----------------------------\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM dbo.DimDate\")\n",
    "    existing_count = cursor.fetchone()[0]\n",
    "\n",
    "    if existing_count > 1:\n",
    "        logging.info(f\"DimDate already contains {existing_count} records. Skipping population.\")\n",
    "        #print(f\"DimDate already contains {existing_count} records. Skipping population.\")\n",
    "    else:\n",
    "        logging.info(\"Populating DimDate table for the first time...\")\n",
    "\n",
    "        insert_sql = \"\"\"\n",
    "            INSERT INTO dbo.DimDate (\n",
    "                date_key,\n",
    "                full_date,\n",
    "                year,\n",
    "                month,\n",
    "                month_name,\n",
    "                day,\n",
    "                quarter\n",
    "            )\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure correct order of columns\n",
    "        rows = [\n",
    "            (\n",
    "                int(row.date_key),\n",
    "                row.full_date.date(),  # convert Timestamp -> date\n",
    "                int(row.year),\n",
    "                int(row.month),\n",
    "                str(row.month_name),\n",
    "                int(row.day),\n",
    "                int(row.quarter)\n",
    "            )\n",
    "            for row in dim_date_df.itertuples(index=False)\n",
    "        ]\n",
    "\n",
    "        # Insert in batches\n",
    "        # -----------------------------\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(rows), batch_size):\n",
    "            batch = rows[i:i + batch_size]\n",
    "            cursor.executemany(insert_sql, batch)\n",
    "            conn.commit()\n",
    "            logging.info(f\"Inserted {i + len(batch)} of {len(rows)} DimDate rows so far...\")\n",
    "\n",
    "        logging.info(\"DimDate population completed successfully.\")\n",
    "        #print(\"DimDate population completed successfully.\")\n",
    "\n",
    "        # Log summary stats\n",
    "        min_date = dim_date_df[\"full_date\"].min().date()\n",
    "        max_date = dim_date_df[\"full_date\"].max().date()\n",
    "        logging.info(f\"Inserted dates from {min_date} to {max_date}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during DimDate population: {e}\", exc_info=True)\n",
    "    #print(f\"Error during DimDate population: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e1d66",
   "metadata": {},
   "source": [
    "## Fact Sales incremental data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3b5e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.info(\"Starting incremental 'FactSales' population process...\")\n",
    "\n",
    "    # Select required columns & clean duplicates\n",
    "    # ----------------------------------------------------\n",
    "    fact_sales_df = (\n",
    "        sales_df[\n",
    "            [\n",
    "                \"transaction_id\",\n",
    "                \"product_id\",\n",
    "                \"customer_id\",\n",
    "                \"date\",\n",
    "                \"quantity\",\n",
    "                \"discount\",\n",
    "                \"region\"\n",
    "            ]\n",
    "        ]\n",
    "        .drop_duplicates()   # drop exact duplicate rows\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    logging.info(f\"FactSales DataFrame with {len(fact_sales_df)} rows after dropping duplicates.\")\n",
    "\n",
    "    \n",
    "    # Handle composite key duplicates (transaction_id + product_id)\n",
    "    # ----------------------------------------------------\n",
    "    duplicate_mask = fact_sales_df.duplicated(subset=[\"transaction_id\", \"product_id\"], keep=\"first\")\n",
    "    duplicate_count = duplicate_mask.sum()\n",
    "\n",
    "    if duplicate_count > 0:\n",
    "        bad_duplicate_rows = fact_sales_df[duplicate_mask].copy()\n",
    "        fact_sales_df = fact_sales_df[~duplicate_mask].copy()\n",
    "\n",
    "        logging.warning(f\"Found {duplicate_count} duplicate composite key rows (transaction_id, product_id). Dropping duplicates.\")\n",
    "\n",
    "        # Log to bad data table\n",
    "        log_bad_data(\n",
    "            df_rows=bad_duplicate_rows,\n",
    "            etl_run_id=etl_run_id,\n",
    "            source_system='python_etl',\n",
    "            bad_column='composite_key',\n",
    "            issue_type='duplicate_key',\n",
    "            notes_prefix=f\"Duplicate (transaction_id, product_id) combination found; dropped {duplicate_count} records.\",\n",
    "            conn_str=sql_conn_str\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Logged {duplicate_count} duplicate composite key rows to bad data table.\")\n",
    "    else:\n",
    "        logging.info(\"No duplicate composite keys found in FactSales input data.\")\n",
    "\n",
    "        \n",
    "    # Apply data type casting\n",
    "    # ----------------------------------------------------\n",
    "    fact_sales_df[\"transaction_id\"] = fact_sales_df[\"transaction_id\"].astype(str)\n",
    "    fact_sales_df[\"product_id\"] = fact_sales_df[\"product_id\"].astype(str)\n",
    "    fact_sales_df[\"customer_id\"] = fact_sales_df[\"customer_id\"].astype(str)\n",
    "    fact_sales_df[\"date\"] = fact_sales_df[\"date\"].astype(int)\n",
    "    fact_sales_df[\"quantity\"] = fact_sales_df[\"quantity\"].astype(int)\n",
    "    fact_sales_df[\"discount\"] = fact_sales_df[\"discount\"].astype(float)\n",
    "    fact_sales_df[\"region\"] = fact_sales_df[\"region\"].astype(str)\n",
    "\n",
    "    logging.info(\"Applied column type casting for FactSales DataFrame.\")\n",
    "\n",
    "    \n",
    "    # Ensure customer_id exists in DimCustomer\n",
    "    # ----------------------------------------------------\n",
    "    logging.info(\"Validating customer_id values against DimCustomer...\")\n",
    "\n",
    "    dim_customers = pd.read_sql(\"SELECT customer_id FROM dbo.DimCustomer\", conn)\n",
    "    valid_customers = set(dim_customers[\"customer_id\"].astype(str).tolist())\n",
    "\n",
    "    # Replace missing or invalid customer_ids with 'Unknown'\n",
    "    fact_sales_df[\"customer_id\"] = fact_sales_df[\"customer_id\"].apply(\n",
    "        lambda x: x if x in valid_customers else \"Unknown\"\n",
    "    )\n",
    "\n",
    "    logging.info(\"Replaced non-existing customer_id values with 'Unknown'.\")\n",
    "\n",
    "    # Load existing keys from target FactSales table\n",
    "    # ----------------------------------------------------\n",
    "    logging.info(\"Fetching existing (transaction_id, product_id) keys from FactSales...\")\n",
    "\n",
    "    cursor.execute(\"SELECT transaction_id, product_id FROM dbo.FactSales;\")\n",
    "    existing_keys = set(tuple(row) for row in cursor.fetchall())\n",
    "    logging.info(f\"Fetched {len(existing_keys)} existing FactSales records.\")\n",
    "\n",
    "    # Filter new records (incremental load)\n",
    "    # ----------------------------------------------------\n",
    "    new_rows = fact_sales_df[\n",
    "        ~fact_sales_df.apply(lambda x: (x[\"transaction_id\"], x[\"product_id\"]) in existing_keys, axis=1)\n",
    "    ].copy()\n",
    "\n",
    "    logging.info(f\"{len(new_rows)} new FactSales records identified for insertion.\")\n",
    "\n",
    "    if new_rows.empty:\n",
    "        #print(\"No new FactSales records to insert. Table is already up to date.\")\n",
    "        logging.info(\"No new FactSales records to insert. Table is already up to date.\")\n",
    "    else:\n",
    "        # Insert new records in batches\n",
    "        # ----------------------------------------------------\n",
    "        insert_sql = \"\"\"\n",
    "            INSERT INTO dbo.FactSales (\n",
    "                transaction_id,\n",
    "                product_id,\n",
    "                customer_id,\n",
    "                date_key,\n",
    "                quantity,\n",
    "                discount,\n",
    "                region\n",
    "            )\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?);\n",
    "        \"\"\"\n",
    "\n",
    "        rows = list(new_rows.itertuples(index=False, name=None))\n",
    "        batch_size = 1000\n",
    "\n",
    "        for i in range(0, len(rows), batch_size):\n",
    "            batch = rows[i:i + batch_size]\n",
    "            cursor.executemany(insert_sql, batch)\n",
    "            conn.commit()\n",
    "            logging.info(f\"Inserted {i + len(batch)} of {len(rows)} new FactSales rows so far...\")\n",
    "\n",
    "        logging.info(\"FactSales incremental load completed successfully.\")\n",
    "        #print(\"FactSales incremental load completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during FactSales population: {e}\", exc_info=True)\n",
    "    #print(f\"Error during FactSales population: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7642a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"ETL process completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
